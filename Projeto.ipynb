{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qIZv-QjbUcR"
   },
   "source": [
    "<!-- Projeto Desenvolvido na Universidade Federal do Tocantins -->\n",
    "# Universidade Federal do Tocantins\n",
    "## Inteligência Artificial Para Visão Computacional\n",
    "## Projeto 2\n",
    "### Fine-Tuning de Modelo Pré-Treinado Para Classificação de Imagens de Animais Silvestres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZHZgtgAbUcS"
   },
   "source": [
    "## Instalando e Carregando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL4erZE8bUcU",
    "outputId": "688214ab-153c-490f-c2ac-c83777da9bf8"
   },
   "outputs": [],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X6yiSUCbUcU"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import (CenterCrop,\n",
    "                                    Compose,\n",
    "                                    Normalize,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop,\n",
    "                                    Resize,\n",
    "                                    ToTensor)\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9NpIHrQbUcU"
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Universidade Federal do Tocantins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'onepiece'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcE455KaG687"
   },
   "source": [
    "## Automatizando a Carga dos Seus Próprios Dados Para Ajuste do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp9xJcHP2TTP"
   },
   "outputs": [],
   "source": [
    "# Carrega o dataset no formato zip e descompacta\n",
    "# dados = load_dataset(\"imagefolder\", data_files = f'data/{dataset}.zip')\n",
    "# Carrega o dataset diretamente do Hugging Face Hub\n",
    "\n",
    "dados = load_dataset(\"BangumiBase/onepiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DogmC7vBbUcV",
    "outputId": "26bbd1b6-39f2-4f97-9192-6ed75733ecff"
   },
   "outputs": [],
   "source": [
    "print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aStUZHXLbUcV",
    "outputId": "24a7324f-8bdb-4fd0-8fd4-655b9f48cf71"
   },
   "outputs": [],
   "source": [
    "type(dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVBbpbWhbUcV"
   },
   "source": [
    "## Explorando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDytKQiXbUcV",
    "outputId": "65e24063-7251-4ce7-ba39-c0ef0507c1b5"
   },
   "outputs": [],
   "source": [
    "# Temos imagens e labels no dicionário dados\n",
    "dados[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4bvNyrrbUcV",
    "outputId": "90ed940a-c32d-428a-f0c2-ec8228798bbb"
   },
   "outputs": [],
   "source": [
    "# Detalhes da imagem de índice 1142 (por exemplo)\n",
    "print(dados['train'][1142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2OIBAtXbUcV"
   },
   "outputs": [],
   "source": [
    "# Extraindo a imagem de índice 1142\n",
    "imagem = dados[\"train\"][1142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_gTlOe2bUcV",
    "outputId": "63eec65f-064c-4207-e803-25ca6af3d9c3"
   },
   "outputs": [],
   "source": [
    "# Cada imagem tem a matriz de pixels no formato PIL e o label\n",
    "imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVWXTd_BbUcV",
    "outputId": "f1cea2f9-75a9-4229-9e80-766ddbd75a56"
   },
   "outputs": [],
   "source": [
    "# Imprimindo o label\n",
    "imagem['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81ek7cKcbUcW",
    "outputId": "9a00c980-36d3-4233-d9f2-d0be835d3d83"
   },
   "outputs": [],
   "source": [
    "# Visualizamos a imagem\n",
    "imagem['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1WAM8sCbUcW",
    "outputId": "213234c3-a0c5-4188-e123-cecaba30b870"
   },
   "outputs": [],
   "source": [
    "# Testando o redimensionamento da imagem usando o método resize\n",
    "imagem['image'].resize((800, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbd5Q2enbUcW"
   },
   "source": [
    "O campo `label` não é um rótulo no formato de string. Por padrão, os campos `ClassLabel` são codificados em números inteiros por conveniência, no pacote datasets. Mas podemos extrair o nome de classe assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV60fvUQbUcW",
    "outputId": "7a380690-bb3e-4fc1-9fe8-1a73517a841b"
   },
   "outputs": [],
   "source": [
    "# Nomes dos labels\n",
    "dados[\"train\"].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk3h1a6MbUcW",
    "outputId": "32173b61-d4bc-48a3-da23-fc5e59c5abe0"
   },
   "outputs": [],
   "source": [
    "# Label de índice 1\n",
    "dados[\"train\"].features['label'].names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHa5nRpObUcW"
   },
   "source": [
    "## Criando Mapeamentos Índice/Nome de Classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LdpDtScLgeD"
   },
   "source": [
    "Vamos criar um dicionário chamado `id2label` para decodificar os ids de classes em strings. O `label2id` inverso também será útil quando carregarmos o modelo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52mjhu3_bUcW"
   },
   "outputs": [],
   "source": [
    "# Extrai os nomes de labels\n",
    "labels = dados[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8SQ0SANbUcW"
   },
   "outputs": [],
   "source": [
    "# Gera os objetos para os mapeamentos\n",
    "label2id, id2label = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75I4pcJQbUcW"
   },
   "outputs": [],
   "source": [
    "# Loop carregar os mapeamentos\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "UuyXDtQqNUZW",
    "outputId": "5f7627b3-6a59-49f6-baaa-4ead1923f728"
   },
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ2rI-7QbUcW",
    "outputId": "ade41edc-9fe9-4a06-b8d0-92f1b88f2c6b"
   },
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zxoikSOjs0K"
   },
   "source": [
    "## Pré-Processamento das Imagens\n",
    "\n",
    "https://huggingface.co/google/vit-base-patch16-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkSvWwnUbUcX"
   },
   "outputs": [],
   "source": [
    "# Nome do repositório no HF\n",
    "dsa_modelo_hf = \"google/vit-base-patch16-224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1bX4lGAO_d9"
   },
   "outputs": [],
   "source": [
    "# Import do processador de imagens usado no treinamento do modelo no HF\n",
    "dsa_image_processor = AutoImageProcessor.from_pretrained(dsa_modelo_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRc-QLoQbUcX",
    "outputId": "9d6d13cf-abca-476b-bdaf-135394e23b27"
   },
   "outputs": [],
   "source": [
    "dsa_image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUtxmoMvqml1"
   },
   "source": [
    "Aqui definimos 2 funções separadas, uma para treinamento (que inclui aumento de dados) e outra para validação (que inclui apenas redimensionamento, corte central e normalização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1LvubClbUca"
   },
   "outputs": [],
   "source": [
    "# Normalização das imagens\n",
    "dsa_normalize = Normalize(mean = dsa_image_processor.image_mean, std = dsa_image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsIi-beFbUca",
    "outputId": "62b34529-f6fb-438f-c547-d84ce1a566fe"
   },
   "outputs": [],
   "source": [
    "type(dsa_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljEe8Ah6bUca"
   },
   "source": [
    "Vamos extrair alguns detalhes do processador de imagens e usar isso ao preparar as transformações para nossas próprias imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnDVWmoWbUca"
   },
   "outputs": [],
   "source": [
    "# Verifica se a chave 'height' está presente no dicionário 'size' do objeto 'dsa_image_processor'\n",
    "if \"height\" in dsa_image_processor.size:\n",
    "\n",
    "    # Se 'height' está presente, define 'size' como uma tupla contendo a altura e largura\n",
    "    size = (dsa_image_processor.size[\"height\"], dsa_image_processor.size[\"width\"])\n",
    "\n",
    "    # Define 'crop_size' igual ao 'size' definido anteriormente\n",
    "    crop_size = size\n",
    "\n",
    "    # Define 'max_size' como None, pois não é especificado neste ramo da condição\n",
    "    max_size = None\n",
    "\n",
    "# Verifica se a chave 'shortest_edge' está presente no dicionário 'size' do objeto 'dsa_image_processor'\n",
    "elif \"shortest_edge\" in dsa_image_processor.size:\n",
    "\n",
    "    # Se 'shortest_edge' está presente, define 'size' como o valor de 'shortest_edge'\n",
    "    size = dsa_image_processor.size[\"shortest_edge\"]\n",
    "\n",
    "    # Define 'crop_size' como uma tupla com ambos os valores sendo 'size'\n",
    "    crop_size = (size, size)\n",
    "\n",
    "    # Define 'max_size' como o valor de 'longest_edge' ou None se 'longest_edge' não existir\n",
    "    max_size = dsa_image_processor.size.get(\"longest_edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u8Tjo-SbUca"
   },
   "outputs": [],
   "source": [
    "# Cria a composição das transformações nos dados de treino\n",
    "transformacoes_treino = Compose([RandomResizedCrop(crop_size), RandomHorizontalFlip(), ToTensor(), dsa_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CEFKs0LbUca"
   },
   "outputs": [],
   "source": [
    "# Cria a composição das transformações nos dados de validação/teste\n",
    "transformacoes_valid = Compose([Resize(size), CenterCrop(crop_size), ToTensor(), dsa_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75mEfJ8sbUca"
   },
   "outputs": [],
   "source": [
    "# Função de pré-processamento de dados de treino\n",
    "def dsa_preprocessa_treino(lote_dados):\n",
    "\n",
    "    lote_dados[\"pixel_values\"] = [transformacoes_treino(image.convert(\"RGB\")) for image in lote_dados[\"image\"]]\n",
    "\n",
    "    return lote_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4O_p3WrpRyej"
   },
   "outputs": [],
   "source": [
    "# Função de pré-processamento de dados de validação/teste\n",
    "def dsa_preprocessa_valid(lote_dados):\n",
    "\n",
    "    lote_dados[\"pixel_values\"] = [transformacoes_valid(image.convert(\"RGB\")) for image in lote_dados[\"image\"]]\n",
    "\n",
    "    return lote_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF4O0KFBGXir"
   },
   "source": [
    "A seguir, podemos pré-processar nosso conjunto de dados aplicando essas funções. Usaremos a funcionalidade `set_transform`, que permite aplicar as funções acima on-the-fly (ou seja, elas só serão aplicadas quando as imagens forem carregadas na memória RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P13tqfFTZ_F4"
   },
   "outputs": [],
   "source": [
    "# Vamos criar o índice para dividir os dados de treino em treino e validação\n",
    "splits = dados[\"train\"].train_test_split(test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5An8ndEObUcb"
   },
   "outputs": [],
   "source": [
    "# Dados de treino\n",
    "dados_treino = splits['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o7Eh9qsbUcb"
   },
   "outputs": [],
   "source": [
    "# Aplica o pré-processamento\n",
    "dados_treino.set_transform(dsa_preprocessa_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ng9TAlDV8d7r",
    "outputId": "ea1be226-1d87-4146-c2c7-5269df8eca49"
   },
   "outputs": [],
   "source": [
    "# Matriz de pixels e label da imagem de índice 10\n",
    "dados_treino[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwO4m5tzbUcb"
   },
   "outputs": [],
   "source": [
    "# Dados de validação\n",
    "dados_valid = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUs56-mprQi1"
   },
   "outputs": [],
   "source": [
    "# Aplica o pré-processamento\n",
    "dados_valid.set_transform(dsa_preprocessa_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaU1NVxzbUcb",
    "outputId": "f6513acb-d9b4-4f26-ca47-61a32420ca88"
   },
   "outputs": [],
   "source": [
    "# Matriz de pixels e label da imagem de índice 23\n",
    "dados_valid[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOXmyPQ76Qv9"
   },
   "source": [
    "## Definindo Argumentos e Hiperparâmetros do Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a-2YT7O6ayC"
   },
   "source": [
    "Agora que nossos dados estão prontos, podemos baixar o modelo pré-treinado e ajustá-lo.\n",
    "\n",
    "Para classificação usamos a classe `AutoModelForImageClassification`. Chamar o método `from_pretrained` fará o download e armazenará em cache os pesos do modelo.\n",
    "\n",
    "Como os IDs dos rótulos e o número de rótulos dependem do conjunto de dados, passamos `label2id` e `id2label` junto com o repositório para download do modelo pré-treinado. Isso garantirá que um cabeçalho de classificação personalizado seja criado (com um número personalizado de neurônios de saída)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9DDujL0q1ac",
    "outputId": "68b09788-53f7-4504-96f5-15c9c61ec6c9"
   },
   "outputs": [],
   "source": [
    "# Carrega o modelo pré-treinado\n",
    "modelo = AutoModelForImageClassification.from_pretrained(dsa_modelo_hf,\n",
    "                                                         label2id = label2id,\n",
    "                                                         id2label = id2label,\n",
    "                                                         ignore_mismatched_sizes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8EmET_f6458"
   },
   "source": [
    "O aviso acima está nos dizendo que estamos descartando alguns pesos (os pesos e bias da camada `classificador`) e inicializando aleatoriamente alguns outros (os pesos e bias de uma nova camada `classificador`). Isso é esperado neste caso, porque estamos adicionando um novo cabeçote para o qual não temos pesos pré-treinados, então a biblioteca nos avisa que devemos ajustar esse modelo antes de usá-lo para inferência, que é exatamente o que vamos fazer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11ZdF8OObUcc"
   },
   "outputs": [],
   "source": [
    "# Pasta para salvar o modelo\n",
    "modelo_salvar = dsa_modelo_hf.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejndUCxdbUcc"
   },
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "batch_size = 32\n",
    "taxa_aprendizado = 5e-5\n",
    "accumulation_steps = 4\n",
    "num_epochs = 3\n",
    "wratio = 0.1\n",
    "lsteps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlH8Z48VbUcc"
   },
   "source": [
    "Veja a descrição completa dos hiperparâmetros acima no Capítulo 6 do curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xc_MTm0Ks3DF"
   },
   "outputs": [],
   "source": [
    "# Argumentos de treino\n",
    "dsa_args = TrainingArguments(f\"{modelo_salvar}-dsa-p2-finetuned\",\n",
    "                             remove_unused_columns = False,\n",
    "                             evaluation_strategy = \"epoch\",\n",
    "                             save_strategy = \"epoch\",\n",
    "                             learning_rate = taxa_aprendizado,\n",
    "                             per_device_train_batch_size = batch_size,\n",
    "                             gradient_accumulation_steps = accumulation_steps,\n",
    "                             per_device_eval_batch_size = batch_size,\n",
    "                             num_train_epochs = num_epochs,\n",
    "                             warmup_ratio = wratio,\n",
    "                             logging_steps = lsteps,\n",
    "                             load_best_model_at_end = True,\n",
    "                             metric_for_best_model = \"accuracy\",\n",
    "                             push_to_hub = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VE_HSha9RZk"
   },
   "source": [
    "A seguir, precisamos definir uma função para calcular as métricas das previsões, que usará apenas a `métrica` que carregamos anteriormente. O único pré-processamento que precisamos fazer é pegar o argmax dos nossos logits previstos.\n",
    "\n",
    "Logits são os valores brutos de saída de uma camada de rede neural antes de serem normalizados por uma função de ativação, como a função softmax em problemas de classificação. Em termos mais técnicos, os logits são as entradas para a última função de ativação de uma rede neural, que é responsável por transformar esses valores brutos em probabilidades.\n",
    "\n",
    "Para entender melhor, considere o contexto de uma rede neural usada para classificação. Na última camada da rede, antes da aplicação da função softmax, você tem um conjunto de valores, cada um correspondendo a uma classe potencial. Estes valores são os logits. Eles podem ser positivos, negativos, grandes ou pequenos, e não estão restritos a um intervalo específico.\n",
    "\n",
    "A função softmax, então, é aplicada a esses logits para transformá-los em probabilidades. A softmax assegura que a soma das probabilidades de todas as classes seja igual a 1, tornando os valores mais interpretáveis e úteis para classificação. Cada logit é transformado em uma probabilidade que representa a confiança do modelo de que a entrada pertence à classe correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI6n_5k-bUcc"
   },
   "outputs": [],
   "source": [
    "# Métrica\n",
    "dsa_metrica = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVWfiBuv2uCS"
   },
   "outputs": [],
   "source": [
    "# Função para calcular as métricas\n",
    "def dsa_compute_metrics(eval_pred):\n",
    "\n",
    "    # Previsões do modelo\n",
    "    predictions = np.argmax(eval_pred.predictions, axis = 1)\n",
    "\n",
    "    # Retorna a métrica\n",
    "    return dsa_metrica.compute(predictions = predictions, references = eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0PqjzHQVutb"
   },
   "source": [
    "Também definimos um `collate_fn`, que será usado para agrupar exemplos. Cada lote consiste em 2 chaves, sendo `pixel_values` e `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0WcwsX7rW9w"
   },
   "outputs": [],
   "source": [
    "# Definição de uma função de collate personalizada para o DataLoader\n",
    "def dsa_collate_fn(examples):\n",
    "\n",
    "    # Agrupa os valores dos pixels de cada exemplo em um batch, usando torch.stack\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "\n",
    "    # Cria um tensor com as labels (etiquetas) de cada exemplo no batch\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "\n",
    "    # Retorna um dicionário contendo os valores dos pixels e as labels correspondentes\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTF0dWw49fB9"
   },
   "source": [
    "> Agora só precisamos passar tudo isso junto com nossos conjuntos de dados para o `Trainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McVoaCPr3Cj-"
   },
   "outputs": [],
   "source": [
    "# Cria o Trainer\n",
    "dsa_trainer = Trainer(modelo,\n",
    "                      dsa_args,\n",
    "                      train_dataset = dados_treino,\n",
    "                      eval_dataset = dados_valid,\n",
    "                      tokenizer = dsa_image_processor,\n",
    "                      compute_metrics = dsa_compute_metrics,\n",
    "                      data_collator = dsa_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j6VNsGP97LG"
   },
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Agora podemos ajustar nosso modelo chamando o método `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp7K4eGUbUcc",
    "outputId": "bd8eaf45-5544-43c7-8e81-bc010342dd2c"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# resultados_treino = dsa_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExelWpM5bUcc"
   },
   "source": [
    "Conseguimos cerca de 94% de acurácia em apenas 18 minutos de treinamento de um incrível modelo de Visão Computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "Pps61vF_4QaH",
    "outputId": "b3e4e3f7-d158-4f74-e3c1-63f464a29096"
   },
   "outputs": [],
   "source": [
    "# # Salvamos modelo e métricas\n",
    "# dsa_trainer.save_model()\n",
    "# dsa_trainer.log_metrics(\"train\", resultados_treino.metrics)\n",
    "# dsa_trainer.save_metrics(\"train\", resultados_treino.metrics)\n",
    "# dsa_trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "modelo_salvo_path = f\"models/{dataset}-trained-model\"\n",
    "\n",
    "if os.path.exists(os.path.join(modelo_salvo_path, \"pytorch_model.bin\")) or os.path.exists(os.path.join(modelo_salvo_path, \"model.safetensors\")):\n",
    "    print(\"Modelo já treinado encontrado. Carregando modelo salvo...\")\n",
    "    modelo = AutoModelForImageClassification.from_pretrained(modelo_salvo_path)\n",
    "else:\n",
    "    print(\"Modelo salvo não encontrado. Treinando modelo...\")\n",
    "    modelo = AutoModelForImageClassification.from_pretrained(\n",
    "        dsa_modelo_hf,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    dsa_trainer = Trainer(\n",
    "        modelo,\n",
    "        dsa_args,\n",
    "        train_dataset=dados_treino,\n",
    "        eval_dataset=dados_valid,\n",
    "        tokenizer=dsa_image_processor,\n",
    "        compute_metrics=dsa_compute_metrics,\n",
    "        data_collator=dsa_collate_fn\n",
    "    )\n",
    "    resultados_treino = dsa_trainer.train()\n",
    "    dsa_trainer.save_model()\n",
    "    dsa_trainer.log_metrics(\"train\", resultados_treino.metrics)\n",
    "    dsa_trainer.save_metrics(\"train\", resultados_treino.metrics)\n",
    "    dsa_trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS60fr0xbUcd"
   },
   "source": [
    "## Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyb-58x_-A0e"
   },
   "source": [
    "Criamos o avaliador e extraímos as métricas de avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugQSA-qvbUcd",
    "outputId": "a1721721-28d9-4ca0-cb3f-41837dd18941"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminho para os resultados de avaliação\n",
    "avaliacao_salva_path = \"models/oregon-wild-life-trained-model/eval_results.json\"\n",
    "\n",
    "if os.path.exists(avaliacao_salva_path):\n",
    "    print(\"Resultados de avaliação encontrados. Carregando resultados salvos...\")\n",
    "    with open(avaliacao_salva_path, \"r\") as f:\n",
    "        avaliador = json.load(f)\n",
    "else:\n",
    "    print(\"Resultados de avaliação não encontrados. Avaliando modelo...\")\n",
    "    avaliador = dsa_trainer.evaluate()\n",
    "    with open(avaliacao_salva_path, \"w\") as f:\n",
    "        json.dump(avaliador, f)\n",
    "\n",
    "print(avaliador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "niniUAnb5IrR",
    "outputId": "8256ef7b-d15b-4cdb-ad41-32804347cb24"
   },
   "outputs": [],
   "source": [
    "# # Extrai as métricas com o avaliador\n",
    "# dsa_trainer.log_metrics(\"eval\", avaliador)\n",
    "# dsa_trainer.save_metrics(\"eval\", avaliador)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmA7biB7bUcd"
   },
   "source": [
    "Conseguimos cerca de 94% de acurácia usando o avaliador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Un6jNBbUcd"
   },
   "source": [
    "## Usando o Modelo Para Previsões com Novas Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUdItyupbUcd"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE52r6bgbUcd"
   },
   "outputs": [],
   "source": [
    "# Carrega a imagem\n",
    "image = Image.open('imagem01.jpeg')\n",
    "#image = Image.open('imagem02.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zBhPr7PbUcd",
    "outputId": "b96c25d3-6317-452d-cc2f-d2f8c6391350"
   },
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHzTlj8EbUcd",
    "outputId": "519fcba2-de32-4977-97ac-dc7a8beb22a2"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img_pil = Image.open('imagem01.jpeg')\n",
    "img_pequena = img_pil.resize((224, 224))\n",
    "display(img_pequena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPzgHp8NbUcd"
   },
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "image = transformacoes_valid(image.convert(\"RGB\")).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icxIUZuabUce"
   },
   "outputs": [],
   "source": [
    "# Move image to the same device as the model\n",
    "image = image.to(modelo.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYtH7VKNbUce"
   },
   "outputs": [],
   "source": [
    "# Faz a previsão\n",
    "with torch.no_grad():\n",
    "    logits = modelo(image).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLdPDECBbUce",
    "outputId": "c14bb981-a6aa-41b8-bc89-b45311937f5d"
   },
   "outputs": [],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie-hIEbIbUce"
   },
   "outputs": [],
   "source": [
    "# Get predicted label\n",
    "id_label_previsto = logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kHyDVdybUce"
   },
   "outputs": [],
   "source": [
    "# Convert label id to label name\n",
    "nome_label_previsto = id2label[id_label_previsto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SkMLU_FFr3B",
    "outputId": "bb7c521f-7bfd-48a9-9959-92ce5a43f360"
   },
   "outputs": [],
   "source": [
    "print(f\"Label Previsto: {nome_label_previsto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Du0OLzvEniCH",
    "outputId": "b23bd127-7f4d-49b8-db19-39a04dd42dd4"
   },
   "outputs": [],
   "source": [
    "%watermark -a \"Universidade Federal do Tocantins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DWPqzGebUce"
   },
   "outputs": [],
   "source": [
    "#%watermark -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEv2zkeVbUce"
   },
   "outputs": [],
   "source": [
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMoq1lG_bUce"
   },
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "projeto2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
